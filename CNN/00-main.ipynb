{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41a1e080-12ef-420b-8923-e789918ba52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2,os\n",
    "import torch,torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1f87fc8-d68a-4fd7-9161-0db9146ad6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models,transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c66946e2-d9d2-4c5e-b35c-2e457c68ff92",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.CenterCrop(112),\n",
    "        transforms.RandomCrop(112),\n",
    "        transforms.RandomRotation(180),\n",
    "        transforms.RandomRotation(90),\n",
    "        transforms.ColorJitter(brightness=0.5,contrast=0.5),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "truth = torch.tensor([0,1,0,0,0,1,1,1])\n",
    "labels_reverse = {0:'not_intrested',1:'intrested'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58458677-f664-4068-97c2-da9e4cb89ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f987b578-8bed-44ea-a3ef-53df38df58df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be03425e-5a32-4b70-85d8-fe3792268dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(img_size=112):\n",
    "    data = []\n",
    "    index = -1\n",
    "    labels = {}\n",
    "    for label in os.listdir('./data/'):\n",
    "        index += 1\n",
    "        labels[f'./data/{label}/'] = [index,-1]\n",
    "    if 'data.npy' in os.listdir('./'):\n",
    "        return labels\n",
    "    for label in labels:\n",
    "        for file in os.listdir(label):\n",
    "            try:\n",
    "                file = label + file\n",
    "                img = cv2.imread(file)\n",
    "                img = cv2.resize(img,(img_size,img_size))\n",
    "                data.append([\n",
    "                    np.array(transformation(np.array(Image.fromarray(np.array(img))))),\n",
    "                    labels[label][0]\n",
    "                ])\n",
    "                labels[label][1] += 1\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    for _ in range(25):\n",
    "        np.random.shuffle(data)\n",
    "    X = []\n",
    "    y = []\n",
    "    for d in data:\n",
    "        X.append(d[0])\n",
    "        y.append(d[1])\n",
    "    VAL_SPLIT = int(len(data) * 0.03125)\n",
    "    X_train = X[:-VAL_SPLIT]\n",
    "    y_train = y[:-VAL_SPLIT]\n",
    "    X_test = X[-VAL_SPLIT:]\n",
    "    y_test = y[-VAL_SPLIT:]\n",
    "    X_train = torch.tensor(X_train)\n",
    "    X_test = torch.tensor(X_test)\n",
    "    y_train = torch.tensor(y_train)\n",
    "    y_test = torch.tensor(y_test)\n",
    "    return [data,X,y,X_train,y_train,X_test,y_test,labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac74228c-23cf-43ce-ab0f-d18122ab085b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data,X,y,X_train,y_train,X_test,y_test,labels = load_data()\n",
    "# with-transformation\n",
    "labels = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1b9ed1e-6a57-4042-832c-d33db69ff71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('./data.npy',data)\n",
    "# # X_train,X_test,y_train,y_test = other_loading_data_proccess(data)\n",
    "# np.save('./X_train.npy',np.array(X_train))\n",
    "# np.save('./y_train.npy',np.array(y_train))\n",
    "# np.save('./X_test.npy',np.array(X_test))\n",
    "# np.save('./y_test.npy',np.array(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fde92d3-2af4-41a1-9e6f-aef452fe0770",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('./data.npy',allow_pickle=True)\n",
    "X_train = torch.from_numpy(np.load('./X_train.npy'))\n",
    "y_train = torch.from_numpy(np.load('./y_train.npy'))\n",
    "X_test = torch.from_numpy(np.load('./X_test.npy'))\n",
    "y_test = torch.from_numpy(np.load('./y_test.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9533452e-b608-495d-a1b2-bd8adfab1273",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1537306-c187-4f1d-bc68-07ad7f204e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 112"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a6f88f5-599c-46e6-9256-6cdfe67d7041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'./data/no_like/': [0, -1], './data/like/': [1, -1]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b00ad598-3279-4c04-90e1-47077beb8ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(net, X, y,device='cpu',transformed=False):\n",
    "    device = device\n",
    "    net.to(device)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(len(X))):\n",
    "            real_class = y[i].to(device)\n",
    "            if transformed:\n",
    "                try:\n",
    "                    net_out = net(\n",
    "                        torch.tensor(np.array(transformation(np.array(Image.fromarray(np.array(X[i]*255).reshape(112,112,3).astype(np.uint8)))))).view(-1,3,IMG_SIZE,IMG_SIZE).to(device).float()\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    net_out = net(\n",
    "                        torch.tensor(np.array(transformation(np.array(Image.fromarray(np.array(X[i])))))).view(-1,3,IMG_SIZE,IMG_SIZE).to(device).float()\n",
    "                    )\n",
    "            else:\n",
    "                net_out = net(\n",
    "                    X[i].view(-1,3,IMG_SIZE,IMG_SIZE).to(device).float()\n",
    "                )\n",
    "            preds = net_out[0][0]\n",
    "            preds = float(preds)\n",
    "            preds = round(preds)\n",
    "            if preds == real_class:\n",
    "                print(f'Pred : {preds}')\n",
    "                print(f'Real : {real_class}')\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    net.train()\n",
    "    print(correct)\n",
    "    print(total)\n",
    "    return round(correct / total, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04d0c1ae-3625-4221-aa57-10997b4bd740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_preds_out(preds, y):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(len(y))):\n",
    "            real_class = y[i].to(device)\n",
    "            pred = preds[i]\n",
    "            pred = float(pred)\n",
    "            pred = round(pred)\n",
    "            if pred == real_class:\n",
    "                print(f'Pred : {pred}')\n",
    "                print(f'Real : {real_class}')\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    print(correct)\n",
    "    print(total)\n",
    "    return round(correct / total, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c66d9dc-1cc9-4516-a9fb-a5309d6f25c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(model='',shape=False,transformed=False,mode='eval',directory='./1st-try/test_data_old/',preds=False):\n",
    "    imgs = []\n",
    "    model.eval()\n",
    "    preds_list = []\n",
    "    idx = -1\n",
    "    from PIL import Image\n",
    "    import face_recognition\n",
    "    with torch.no_grad():\n",
    "        for path in os.listdir(directory):\n",
    "            idx += 1\n",
    "            path = f\"{directory}/{path}\"\n",
    "            image = face_recognition.load_image_file(path)\n",
    "            face_locations = face_recognition.face_locations(image)\n",
    "            for fl in face_locations:\n",
    "                color = (0, 0, 255)\n",
    "                image_path = path\n",
    "                image = cv2.imread(image_path)\n",
    "            if len(face_locations) > 0:\n",
    "                for face_location in face_locations:\n",
    "                    im = Image.open(fr\"{path}\")\n",
    "                    left = face_location[3]\n",
    "                    top = face_location[0]\n",
    "                    right = face_location[1]\n",
    "                    bottom = face_location[2]\n",
    "                    im1 = im.crop((left, top, right, bottom))\n",
    "                    im1.save('./test.png')\n",
    "                    img = cv2.imread('./test.png')\n",
    "                    img = cv2.resize(img,(112,112))\n",
    "                    imgs.append(img)\n",
    "                    model.to(device)\n",
    "                    if transformed:\n",
    "                        preds = model(torch.tensor(np.array(transformation(np.array(Image.fromarray(np.array(img)))))).view(-1,3,112,112).to(device).float())\n",
    "                    else:\n",
    "                        preds = model(torch.tensor(img).view(-1,3,112,112).to(device).float())\n",
    "                    preds = preds[0][0]\n",
    "                    preds = float(preds)\n",
    "                    preds = round(preds)\n",
    "                    preds_list.append(preds)\n",
    "                    plt.figure(figsize=(10,7))\n",
    "                    plt.imshow(img)\n",
    "                    plt.title(f'{labels_reverse[int(preds)]}')\n",
    "                    plt.savefig(f'./{idx}.png')\n",
    "                    plt.show()\n",
    "    return torch.from_numpy(np.array(imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc9fb496-ee35-45af-be62-2a174a243e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0acc48be-3979-4d71-86a0-6260f401ece8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_pred(model='',shape=False,transformed=False,mode='eval',directory='./1st-try/test_data_old/',preds=False,num_of_iters=250):\n",
    "    imgs = []\n",
    "    model.eval()\n",
    "    preds_dict = {}\n",
    "    preds_all = []\n",
    "    preds_all_final = []\n",
    "    idx_dict = {}\n",
    "    idx = -1\n",
    "    from PIL import Image\n",
    "    import face_recognition\n",
    "    with torch.no_grad():\n",
    "        for path in tqdm(os.listdir(directory)):\n",
    "            path = f\"{directory}/{path}\"\n",
    "            image = face_recognition.load_image_file(path)\n",
    "            face_locations = face_recognition.face_locations(image)\n",
    "            if len(face_locations) > 0:\n",
    "                for face_location in face_locations:\n",
    "                    idx += 1\n",
    "                    for _ in range(num_of_iters):\n",
    "                        im = Image.open(fr\"{path}\")\n",
    "                        left = face_location[3]\n",
    "                        top = face_location[0]\n",
    "                        right = face_location[1]\n",
    "                        bottom = face_location[2]\n",
    "                        im1 = im.crop((left, top, right, bottom))\n",
    "                        im1.save('./test.png')\n",
    "                        img = cv2.imread('./test.png')\n",
    "                        img = cv2.resize(img,(112,112))\n",
    "                        imgs.append(img)\n",
    "                        model.to(device)\n",
    "                        if transformed:\n",
    "                            preds = model(torch.tensor(np.array(transformation(np.array(Image.fromarray(np.array(img)))))).view(-1,3,112,112).to(device).float())\n",
    "                        else:\n",
    "                            preds = model(torch.tensor(img).view(-1,3,112,112).to(device).float())\n",
    "                        preds = preds[0][0]\n",
    "                        preds_all.append(preds)\n",
    "                        preds = float(preds)\n",
    "                        preds = round(preds)\n",
    "                        idx_dict[idx] = img\n",
    "                        try:\n",
    "                            preds_dict[idx][preds]+=1\n",
    "                        except Exception as e:\n",
    "                            preds_dict[idx] = {0:-1,1:-1}\n",
    "                            preds_dict[idx][preds]+=1\n",
    "    for img,log in zip(preds_dict.keys(),preds_dict.values()):\n",
    "        print(log)\n",
    "        best_class = -1\n",
    "        if log[0] < log[1] or log[1] > log[0]:\n",
    "            print('1 is bigger than 0')\n",
    "            best_class = 1\n",
    "        elif log[0] > log[1] or log[1] < log[0]:\n",
    "            print('0 is bigger than 1')\n",
    "            best_class = 0\n",
    "        else:\n",
    "            best_class = random.choice([1,0])\n",
    "        idx = img\n",
    "        img = idx_dict[img]\n",
    "        preds_all_final.append(best_class)\n",
    "        plt.figure(figsize=(10,7))\n",
    "        plt.imshow(img)\n",
    "        plt.title(f'{labels_reverse[int(best_class)]}')\n",
    "        plt.savefig(f'./{idx}.png')\n",
    "        plt.show()\n",
    "    return preds_dict,idx_dict,preds_all,preds_all_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03d22788-16c5-46eb-9107-bf27b9321d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "777246f8-0039-440e-9baa-3a667300342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Test(nn.Module):\n",
    "    def __init__(self,num_of_fc_layers=2,activation=F.relu,num_of_conv_layers=1,conv1_out=16,conv2_out=32,fc1_out=64,fc2_out=128):\n",
    "        super().__init__()\n",
    "        print(num_of_fc_layers)\n",
    "        print(activation)\n",
    "        print(num_of_conv_layers)\n",
    "        print(conv1_out)\n",
    "        print(conv2_out)\n",
    "        print(fc1_out)\n",
    "        print(fc2_out)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.conv1 = nn.Conv2d(3, conv1_out, 5)\n",
    "        self.conv1batchnorm = nn.BatchNorm2d(conv1_out)\n",
    "        self.conv2 = nn.Conv2d(conv1_out, conv2_out, 5)\n",
    "        self.conv2batchnorm = nn.BatchNorm2d(conv2_out)\n",
    "        self.conv3 = nn.Conv2d(conv2_out, conv2_out, 5)\n",
    "        self.conv3batchnorm = nn.BatchNorm2d(conv2_out)\n",
    "        self.fc1 = nn.Linear(conv2_out*10*10, fc1_out)\n",
    "        self.fc1atchnorm = nn.BatchNorm1d(fc1_out)\n",
    "        self.fc2 = nn.Linear(fc1_out, fc2_out)\n",
    "        self.fc2atchnorm = nn.BatchNorm1d(fc2_out)\n",
    "        self.fc4 = nn.Linear(fc2_out,fc2_out)\n",
    "        self.fc4batchnorm = nn.BatchNorm1d(fc2_out)\n",
    "        self.fc3 = nn.Linear(fc2_out, 1)\n",
    "        self.activation = activation\n",
    "        self.num_of_fc_layers = num_of_fc_layers\n",
    "        self.conv2_out = conv2_out\n",
    "        self.num_of_conv_layers = num_of_conv_layers\n",
    "\n",
    "    def forward(self, x,shape=False):\n",
    "        x = self.pool(self.activation(self.conv1batchnorm(self.conv1(x))))\n",
    "        x = self.pool(self.activation(self.conv2batchnorm(self.conv2(x))))\n",
    "        for _ in range(self.num_of_conv_layers):\n",
    "            x = self.pool(torch.tensor(self.activation(self.conv3batchnorm(self.conv3(x)))))\n",
    "        if shape:\n",
    "            print(x.shape)\n",
    "        x = x.view(-1, self.conv2_out*10*10)\n",
    "        x = self.activation(self.fc1atchnorm(self.fc1(x)))\n",
    "        x = self.activation(self.fc2atchnorm(self.fc2(x)))\n",
    "        for _ in range(self.num_of_fc_layers):\n",
    "            x = self.activation(self.fc4batchnorm(self.fc4(x)))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b71f6b88-6a28-4a43-83d5-b4b981e653df",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9df54af-49a3-4fb3-99f1-f2d9908b9f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 125+31\n",
    "# epochs = 200\n",
    "# epochs = 250 + 125 + 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "daa617cf-4fab-4498-8e83-b293653a3763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e9d1de7-2abf-425b-8a39-8ff61e940aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "647a84dc-138f-45d0-958c-d2900a77782f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CNN_Test().to(device)\n",
    "# model.to(device)\n",
    "# criterion = nn.BCELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "# wandb.init(project='Intred-or-Not-Product',name='test-cnn-without-transformation')\n",
    "# for epoch in tqdm(range(epochs)):\n",
    "#     torch.cuda.empty_cache()\n",
    "#     for i in range(0,len(X_train),BATCH_SIZE):\n",
    "#         optimizer.zero_grad()\n",
    "#         torch.cuda.empty_cache()\n",
    "#         X_batch = X_train[i:i+BATCH_SIZE].view(-1,3,112,112).to(device)\n",
    "#         y_batch = y_train[i:i+BATCH_SIZE].to(device)\n",
    "#         model.to(device)\n",
    "#         preds = model(X_batch)\n",
    "#         loss = criterion(preds.view(-1,1).float(),y_batch.view(-1,1).float())\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         wandb.log({'loss_per_iter':loss.item()})\n",
    "#     wandb.log({'loss_per_epoch':loss.item()})\n",
    "# torch.cuda.empty_cache()\n",
    "# test_acc_mean = np.mean([accuracy(model,get_image(model=model,shape=True,transformed=True,mode='eval'),truth,transformed=True),accuracy(model,get_image(model=model,shape=True,transformed=False,mode='eval'),truth,transformed=False),accuracy(model,get_image(model=model,shape=True,transformed=False,mode='eval'),truth,transformed=False),accuracy(model,get_image(model=model,shape=True,transformed=False,mode='eval'),truth,transformed=True)])\n",
    "# wandb.log({'test_accuracy':test_acc_mean})\n",
    "# torch.cuda.empty_cache()\n",
    "# wandb.log({'val_accuracy':accuracy(model,X_test,y_test)})\n",
    "# torch.cuda.empty_cache()\n",
    "# wandb.log({'accuracy':accuracy(model,X_train[:1250],y_train[:1250])})\n",
    "# torch.cuda.empty_cache()\n",
    "# wandb.log({'val_accuracy':accuracy(model,X_test,y_test)})\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84e5612d-a4f2-48e6-9dec-86c5072e52c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 = lef\n",
    "# 0 = top\n",
    "# 1 = right\n",
    "# 2 = bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15f02456-927d-4e67-8bd8-45a53a433dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TL_Model_Test(nn.Module):\n",
    "    def __init__(self,model=models.resnet18):\n",
    "        super().__init__()\n",
    "        self.model = model(pretrained=True)\n",
    "#         self.model_v1_to_model_v2 = nn.Linear(1000,1204224)\n",
    "#         self.model_v2 = model(pretrained=True)\n",
    "#         self.output_1 = nn.Linear(1000,500)\n",
    "#         self.relu = F.relu\n",
    "#         self.output_1batchnorm = nn.BatchNorm1d(500)\n",
    "#         self.output_2 = nn.Linear(500,750)\n",
    "#         self.output_2batchnorm = nn.BatchNorm1d(750)\n",
    "#         self.output_3 = nn.Linear(750,2)\n",
    "        self.output = nn.Linear(1000,1)\n",
    "    def forward(self,X):\n",
    "        preds = self.model(X)\n",
    "#         preds = self.model_v1_to_model_v2(preds)\n",
    "#         preds = preds.view(X.shape)\n",
    "#         preds = self.model_v2(preds)\n",
    "#         preds = self.relu(self.output_1batchnorm(self.output_1(preds)))\n",
    "#         preds = self.relu(self.output_2batchnorm(self.output_2(preds)))\n",
    "#         preds = self.output_3(preds)\n",
    "        preds = torch.sigmoid(self.output(preds))\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81d2805b-067c-4591-881e-2a7d9e0c23bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = TL_Model_Test().to(device)\n",
    "# model.to(device)\n",
    "# criterion = nn.BCELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "# wandb.init(project='Intred-or-Not-Product',name='test-tl_model-without-transformation')\n",
    "# for epoch in tqdm(range(epochs)):\n",
    "#     for i in range(0,len(X_train),BATCH_SIZE):\n",
    "#         optimizer.zero_grad()\n",
    "#         X_batch = X_train[i:i+BATCH_SIZE].view(-1,3,112,112).to(device)\n",
    "#         y_batch = y_train[i:i+BATCH_SIZE].to(device)\n",
    "#         model.to(device)\n",
    "#         preds = model(X_batch)\n",
    "#         preds.to(device)\n",
    "#         loss = criterion(preds.view(-1,1).float(),y_batch.view(-1,1).float())\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         wandb.log({'loss_per_iter':loss.item()})\n",
    "#     wandb.log({'loss_per_epoch':loss.item()})\n",
    "# torch.cuda.empty_cache()\n",
    "# test_acc_mean = np.mean([accuracy(model,get_image(model=model,shape=True,transformed=True,mode='eval'),truth,transformed=True),accuracy(model,get_image(model=model,shape=True,transformed=False,mode='eval'),truth,transformed=False),accuracy(model,get_image(model=model,shape=True,transformed=False,mode='eval'),truth,transformed=False),accuracy(model,get_image(model=model,shape=True,transformed=False,mode='eval'),truth,transformed=True)])\n",
    "# wandb.log({'test_accuracy':test_acc_mean})\n",
    "# torch.cuda.empty_cache()\n",
    "# wandb.log({'val_accuracy':accuracy(model,X_test,y_test)})\n",
    "# torch.cuda.empty_cache()\n",
    "# wandb.log({'accuracy':accuracy(model,X_train[:1250],y_train[:1250])})\n",
    "# torch.cuda.empty_cache()\n",
    "# wandb.log({'val_accuracy':accuracy(model,X_test,y_test)})\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8585ef10-84ce-4466-936f-e5ae7d193069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet18 = [models.resnet18,'resnet18']\n",
    "# resnet34 = [models.resnet34,'resnet34']\n",
    "# resnet50 = [models.resnet50,'resnet50']\n",
    "# resnet101 = [models.resnet101,'resnet101']\n",
    "# resnet152 = [models.resnet152,'resnet152']\n",
    "# resnext50_32x4d = [models.resnext50_32x4d,'resnext50_32x4d']\n",
    "# resnext101_32x8d = [models.resnext101_32x8d,'resnext101_32x8d']\n",
    "# wide_resnet50_2 = [models.wide_resnet50_2,'wide_resnet50_2']\n",
    "# wide_resnet101_2 = [models.wide_resnet101_2,'wide_resnet101_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5457a9bc-8290-4f14-9bde-1b6133ca9150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_of_fc_layers = \n",
    "# activation\n",
    "# num_of_conv_layers\n",
    "# conv1_out\n",
    "# conv2_out\n",
    "# fc1_out\n",
    "# fc2_out\n",
    "# optimizer\n",
    "# criterion\n",
    "# img size\n",
    "# epochs\n",
    "# batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f569f69-9a84-4eca-a54e-1302b647e477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_of_fc_layers = [1,2,3,4,5]\n",
    "# for num_of_fc_layer in num_of_fc_layers:\n",
    "#     model = Test_CNN(num_of_fc_layer=num_of_fc_layer).to(device)\n",
    "#     model.to(device)\n",
    "#     criterion = nn.BCELoss()\n",
    "#     optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "#     wandb.init(project='Intred-or-Not-Product',name=f'{num_of_fc_layer}-num_of_fc_layer')\n",
    "#     for epoch in tqdm(range(epochs)):\n",
    "#         for i in range(0,len(X_train),BATCH_SIZE):\n",
    "#             X_batch = X_train[i:i+BATCH_SIZE].view(-1,3,112,112).to(device)\n",
    "#             y_batch = y_train[i:i+BATCH_SIZE].to(device)\n",
    "#             model.to(device)\n",
    "#             preds = model(X_batch)\n",
    "#             preds.to(device)\n",
    "#             loss = criterion(preds.view(-1,1).float(),y_batch.view(-1,1).float())\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             wandb.log({'loss_per_iter':loss.item()})\n",
    "#         wandb.log({'loss_per_epoch':loss.item()})\n",
    "#     test_acc_mean = np.mean([accuracy(model,get_image(model=model,shape=True,transformed=True,mode='eval'),truth,transformed=True),accuracy(model,get_image(model=model,shape=True,transformed=False,mode='eval'),truth,transformed=False),accuracy(model,get_image(model=model,shape=True,transformed=False,mode='eval'),truth,transformed=False),accuracy(model,get_image(model=model,shape=True,transformed=False,mode='eval'),truth,transformed=True)])\n",
    "#     print(test_acc_mean)\n",
    "#     torch.cuda.empty_cache()\n",
    "#     wandb.log({'test_accuracy':test_acc_mean})\n",
    "#     torch.cuda.empty_cache()\n",
    "#     wandb.log({'val_accuracy':accuracy(model,X_test,y_test)})\n",
    "#     torch.cuda.empty_cache()\n",
    "#     wandb.log({'accuracy':accuracy(model,X_train[:1250],y_train[:1250])})\n",
    "#     torch.cuda.empty_cache()\n",
    "#     wandb.log({'val_accuracy':accuracy(model,X_test,y_test)})\n",
    "#     wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c30dc6e3-9eb7-48a1-a18b-a80c402f401a",
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet = [models.alexnet,'alexnet']\n",
    "squeezenet = [models.squeezenet1_0,'squeezenet1_0']\n",
    "googlenet = [models.googlenet,'googlenet']\n",
    "shufflenet = [models.shufflenet_v2_x1_0,'shufflenet_v2_x1_0']\n",
    "resnet34 = [models.resnet34,'resnet34']\n",
    "mobilenet_v2 = [models.mobilenet_v2,'mobilenet_v2']\n",
    "mnasnet0_75 = [models.mnasnet0_75,'mnasnet0_75']\n",
    "densenet161 = [models.densenet161,'densenet161']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e9b25982-e6ed-410d-a220-7ec4513e3c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "162233b9-e8df-4acf-a6e6-f71438b02f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # \n",
    "# models_ = [shufflenet,googlenet,squeezenet,[models.resnet18,'resnet18'],mobilenet_v2,densenet161,alexnet]\n",
    "# for model in models_:\n",
    "#     model_name = model\n",
    "#     model = TL_Model_Test(model=model[0]).to(device)\n",
    "#     model.to(device)\n",
    "#     criterion = nn.BCELoss()\n",
    "#     optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "#     wandb.init(project='Intred-or-Not-Product',name=f'{model_name[1]}-family')\n",
    "#     for epoch in tqdm(range(epochs)):\n",
    "#         for i in range(0,len(X_train),BATCH_SIZE):\n",
    "#             optimizer.zero_grad()\n",
    "#             X_batch = X_train[i:i+BATCH_SIZE].view(-1,3,112,112).to(device)\n",
    "#             y_batch = y_train[i:i+BATCH_SIZE].to(device)\n",
    "#             model.to(device)\n",
    "#             preds = model(X_batch)\n",
    "#             preds.to(device)\n",
    "#             loss = criterion(preds.view(-1,1).float(),y_batch.view(-1,1).float())\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             wandb.log({'loss_per_iter':loss.item()})\n",
    "#         wandb.log({'loss_per_epoch':loss.item()})\n",
    "#     torch.cuda.empty_cache()\n",
    "#     test_acc_mean = np.mean([accuracy(model,get_image(model=model,shape=True,transformed=True,mode='eval'),truth,transformed=True),accuracy(model,get_image(model=model,shape=True,transformed=False,mode='eval'),truth,transformed=False),accuracy(model,get_image(model=model,shape=True,transformed=False,mode='eval'),truth,transformed=False),accuracy(model,get_image(model=model,shape=True,transformed=False,mode='eval'),truth,transformed=True)])\n",
    "#     wandb.log({'test_accuracy':test_acc_mean})\n",
    "#     torch.cuda.empty_cache()\n",
    "#     val_acc_mean = np.mean([accuracy(model,X_test,y_test,transformed=True),accuracy(model,X_test,y_test,transformed=False)])\n",
    "#     wandb.log({'val_accuracy':val_acc_mean})\n",
    "#     torch.cuda.empty_cache()\n",
    "#     train_acc_mean = np.mean([accuracy(model,X_train[:1250],y_train[:1250],transformed=True),accuracy(model,X_train[:1250],y_train[:1250],transformed=False)])\n",
    "#     wandb.log({'accuracy':train_acc_mean})\n",
    "#     wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fbb4c723-3c94-4ae0-8437-70bb796d11bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# googlenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f34db021-bd79-4ad0-a945-26e4a5413db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # torch.optim.Adadelta,torch.optim.Adagrad,torch.optim.Adam,torch.optim.AdamW,torch.optim.Adamax,torch.optim.ASGD,torch.optim.SGD\n",
    "# optimizers = []\n",
    "# for optimizer in optimizers:\n",
    "#     model_name = model\n",
    "#     model = TL_Model_Test(model=googlenet[0]).to(device)\n",
    "#     model.to(device)\n",
    "#     criterion = nn.BCELoss()\n",
    "#     optimizer = optimizer(model.parameters(),lr=0.001)\n",
    "#     wandb.init(project='Intred-or-Not-Product',name=f'{optimizer}-optimizer')\n",
    "#     for epoch in tqdm(range(epochs)):\n",
    "#         for i in range(0,len(X_train),BATCH_SIZE):\n",
    "#             optimizer.zero_grad()\n",
    "#             X_batch = X_train[i:i+BATCH_SIZE].view(-1,3,112,112).to(device)\n",
    "#             y_batch = y_train[i:i+BATCH_SIZE].to(device)\n",
    "#             model.to(device)\n",
    "#             preds = model(X_batch)\n",
    "#             preds.to(device)\n",
    "#             loss = criterion(preds.view(-1,1).float(),y_batch.view(-1,1).float())\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             wandb.log({'loss_per_iter':loss.item()})\n",
    "#         wandb.log({'loss_per_epoch':loss.item()})\n",
    "#     torch.cuda.empty_cache()\n",
    "#     test_acc_mean = np.mean([accuracy(model,get_image(model=model,shape=True,transformed=True,mode='eval'),truth,transformed=True),accuracy(model,get_image(model=model,shape=True,transformed=False,mode='eval'),truth,transformed=False),accuracy(model,get_image(model=model,shape=True,transformed=False,mode='eval'),truth,transformed=False),accuracy(model,get_image(model=model,shape=True,transformed=False,mode='eval'),truth,transformed=True)])\n",
    "#     wandb.log({'test_accuracy':test_acc_mean})\n",
    "#     torch.cuda.empty_cache()\n",
    "#     val_acc_mean = np.mean([accuracy(model,X_test,y_test,transformed=True),accuracy(model,X_test,y_test,transformed=False)])\n",
    "#     wandb.log({'val_accuracy':val_acc_mean})\n",
    "#     torch.cuda.empty_cache()\n",
    "#     train_acc_mean = np.mean([accuracy(model,X_train[:1250],y_train[:1250],transformed=True),accuracy(model,X_train[:1250],y_train[:1250],transformed=False)])\n",
    "#     wandb.log({'accuracy':train_acc_mean})\n",
    "#     wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b2990f99-becb-4b95-9d5e-b02b45e19c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e80ebfb-e8bc-4aec-9c3e-dad84a474796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lrs = [1.0,0.1,0.01,0.001,0.0001]\n",
    "# for lr in lrs:\n",
    "#     model_name = model\n",
    "#     model = TL_Model_Test(model=googlenet[0]).to(device)\n",
    "#     model.to(device)\n",
    "#     criterion = nn.BCELoss()\n",
    "#     optimizer = torch.optim.Adamax(model.parameters(),lr=lr)\n",
    "#     wandb.init(project='Intred-or-Not-Product',name=f'{lr}-lr')\n",
    "#     for epoch in tqdm(range(epochs)):\n",
    "#         for i in range(0,len(X_train),BATCH_SIZE):\n",
    "#             optimizer.zero_grad()\n",
    "#             X_batch = X_train[i:i+BATCH_SIZE].view(-1,3,112,112).to(device)\n",
    "#             y_batch = y_train[i:i+BATCH_SIZE].to(device)\n",
    "#             model.to(device)\n",
    "#             preds = model(X_batch)\n",
    "#             preds.to(device)\n",
    "#             loss = criterion(preds.view(-1,1).float(),y_batch.view(-1,1).float())\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             wandb.log({'loss_per_iter':loss.item()})\n",
    "#         wandb.log({'loss_per_epoch':loss.item()})\n",
    "#     torch.cuda.empty_cache()\n",
    "#     test_acc_mean = np.mean([accuracy(model,get_image(model=model,shape=True,transformed=True,mode='eval'),truth,transformed=True),accuracy(model,get_image(model=model,shape=True,transformed=False,mode='eval'),truth,transformed=False),accuracy(model,get_image(model=model,shape=True,transformed=False,mode='eval'),truth,transformed=False),accuracy(model,get_image(model=model,shape=True,transformed=False,mode='eval'),truth,transformed=True)])\n",
    "#     wandb.log({'test_accuracy':test_acc_mean})\n",
    "#     torch.cuda.empty_cache()\n",
    "#     val_acc_mean = np.mean([accuracy(model,X_test,y_test,transformed=True),accuracy(model,X_test,y_test,transformed=False)])\n",
    "#     wandb.log({'val_accuracy':val_acc_mean})\n",
    "#     torch.cuda.empty_cache()\n",
    "#     train_acc_mean = np.mean([accuracy(model,X_train[:1250],y_train[:1250],transformed=True),accuracy(model,X_train[:1250],y_train[:1250],transformed=False)])\n",
    "#     wandb.log({'accuracy':train_acc_mean})\n",
    "#     wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9e50b9f0-1ab4-4545-b90d-8346882b85bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # nn.MSELoss,nn.L1Loss,torch.nn.HingeEmbeddingLoss,\n",
    "# criterions = [torch.nn.MarginRankingLoss,nn.BCELoss]\n",
    "# for criterion in criterions:\n",
    "#     model = TL_Model_Test(model=googlenet[0]).to(device)\n",
    "#     model.to(device)\n",
    "#     criterion = criterion()\n",
    "#     optimizer = torch.optim.Adamax(model.parameters(),lr=0.0001)\n",
    "#     wandb.init(project='Intred-or-Not-Product',name=f'{criterion}-criterion')\n",
    "#     for epoch in tqdm(range(epochs)):\n",
    "#         for i in range(0,len(X_train),BATCH_SIZE):\n",
    "#             optimizer.zero_grad()\n",
    "#             X_batch = X_train[i:i+BATCH_SIZE].view(-1,3,112,112).to(device)\n",
    "#             y_batch = y_train[i:i+BATCH_SIZE].to(device)\n",
    "#             model.to(device)\n",
    "#             preds = model(X_batch)\n",
    "#             preds.to(device)\n",
    "#             loss = criterion(preds.view(-1,1).float(),y_batch.view(-1,1).float())\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             wandb.log({'loss_per_iter':loss.item()})\n",
    "#         wandb.log({'loss_per_epoch':loss.item()})\n",
    "#     torch.cuda.empty_cache()\n",
    "#     test_acc_mean = np.mean([accuracy(model,get_image(model=model,shape=True,transformed=True,mode='eval'),truth,transformed=True),accuracy(model,get_image(model=model,shape=True,transformed=False,mode='eval'),truth,transformed=False),accuracy(model,get_image(model=model,shape=True,transformed=False,mode='eval'),truth,transformed=False),accuracy(model,get_image(model=model,shape=True,transformed=False,mode='eval'),truth,transformed=True)])\n",
    "#     wandb.log({'test_accuracy':test_acc_mean})\n",
    "#     torch.cuda.empty_cache()\n",
    "#     val_acc_mean = np.mean([accuracy(model,X_test,y_test,transformed=True),accuracy(model,X_test,y_test,transformed=False)])\n",
    "#     wandb.log({'val_accuracy':val_acc_mean})\n",
    "#     torch.cuda.empty_cache()\n",
    "#     train_acc_mean = np.mean([accuracy(model,X_train[:1250],y_train[:1250],transformed=True),accuracy(model,X_train[:1250],y_train[:1250],transformed=False)])\n",
    "#     wandb.log({'accuracy':train_acc_mean})\n",
    "#     wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6a1e40ee-90ea-4537-8587-4dac601c47c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BCELoss = 2\n",
    "# MSELoss = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9c28936a-7d24-485d-acc6-ee1a1cdbbd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 16,32,64,128,len(X_train)\n",
    "# batch_sizes = [256,512,1024,2048]\n",
    "# for batch_size in batch_sizes:\n",
    "#     model = TL_Model_Test(model=googlenet[0]).to(device)\n",
    "#     model.to(device)\n",
    "#     criterion = nn.MSELoss()\n",
    "#     optimizer = torch.optim.Adamax(model.parameters(),lr=0.0001)\n",
    "#     wandb.init(project='Intred-or-Not-Product',name=f'{batch_size}-batch_size')\n",
    "#     for epoch in tqdm(range(epochs)):\n",
    "#         for i in range(0,len(X_train),batch_size):\n",
    "#             optimizer.zero_grad()\n",
    "#             X_batch = X_train[i:i+batch_size].view(-1,3,112,112).to(device)\n",
    "#             y_batch = y_train[i:i+batch_size].to(device)\n",
    "#             model.to(device)\n",
    "#             preds = model(X_batch)\n",
    "#             preds.to(device)\n",
    "#             loss = criterion(preds.view(-1,1).float(),y_batch.view(-1,1).float())\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             wandb.log({'loss_per_iter':loss.item()})\n",
    "#         wandb.log({'loss_per_epoch':loss.item()})\n",
    "#     torch.cuda.empty_cache()\n",
    "#     test_acc_mean = np.mean([accuracy(model,get_image(model=model,shape=True,transformed=True,mode='eval'),truth,transformed=True),accuracy(model,get_image(model=model,shape=True,transformed=False,mode='eval'),truth,transformed=False),accuracy(model,get_image(model=model,shape=True,transformed=False,mode='eval'),truth,transformed=False),accuracy(model,get_image(model=model,shape=True,transformed=False,mode='eval'),truth,transformed=True)])\n",
    "#     wandb.log({'test_accuracy':test_acc_mean})\n",
    "#     torch.cuda.empty_cache()\n",
    "#     val_acc_mean = np.mean([accuracy(model,X_test,y_test,transformed=True),accuracy(model,X_test,y_test,transformed=False)])\n",
    "#     wandb.log({'val_accuracy':val_acc_mean})\n",
    "#     torch.cuda.empty_cache()\n",
    "#     train_acc_mean = np.mean([accuracy(model,X_train[:1250],y_train[:1250],transformed=True),accuracy(model,X_train[:1250],y_train[:1250],transformed=False)])\n",
    "#     wandb.log({'accuracy':train_acc_mean})\n",
    "#     wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6abd6909-cbf1-45e1-a3fa-2f296db58272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 128 = 1\n",
    "# 16 = 1\n",
    "# 64 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "21c8434f-a57b-41a8-a55e-0fcc4118a2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = [37,50,100,125,250]\n",
    "# for epoch in epochs:\n",
    "#     model = TL_Model_Test(model=googlenet[0]).to(device)\n",
    "#     model.to(device)\n",
    "#     criterion = nn.MSELoss()\n",
    "#     optimizer = torch.optim.Adamax(model.parameters(),lr=0.0001)\n",
    "#     wandb.init(project='Intred-or-Not-Product',name=f'{epoch}-epoch')\n",
    "#     for epoch in tqdm(range(epoch)):\n",
    "#         for i in range(0,len(X_train),128):\n",
    "#             optimizer.zero_grad()\n",
    "#             X_batch = X_train[i:i+BATCH_SIZE].view(-1,3,112,112).to(device)\n",
    "#             y_batch = y_train[i:i+BATCH_SIZE].to(device)\n",
    "#             model.to(device)\n",
    "#             preds = model(X_batch)\n",
    "#             preds.to(device)\n",
    "#             loss = criterion(preds.view(-1,1).float(),y_batch.view(-1,1).float())\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             wandb.log({'loss_per_iter':loss.item()})\n",
    "#         wandb.log({'loss_per_epoch':loss.item()})\n",
    "#     torch.cuda.empty_cache()\n",
    "#     test_acc_mean = np.mean([accuracy(model,get_image(model=model,shape=True,transformed=True,mode='eval'),truth,transformed=True),accuracy(model,get_image(model=model,shape=True,transformed=False,mode='eval'),truth,transformed=False),accuracy(model,get_image(model=model,shape=True,transformed=False,mode='eval'),truth,transformed=False),accuracy(model,get_image(model=model,shape=True,transformed=False,mode='eval'),truth,transformed=True)])\n",
    "#     wandb.log({'test_accuracy':test_acc_mean})\n",
    "#     torch.cuda.empty_cache()\n",
    "#     val_acc_mean = np.mean([accuracy(model,X_test,y_test,transformed=True),accuracy(model,X_test,y_test,transformed=False)])\n",
    "#     wandb.log({'val_accuracy':val_acc_mean})\n",
    "#     torch.cuda.empty_cache()\n",
    "#     train_acc_mean = np.mean([accuracy(model,X_train[:1250],y_train[:1250],transformed=True),accuracy(model,X_train[:1250],y_train[:1250],transformed=False)])\n",
    "#     wandb.log({'accuracy':train_acc_mean})\n",
    "#     wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dc0848e0-6151-474b-a439-2f1d4c3aca52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# files = os.listdir('./test_data/')\n",
    "# for file in files:\n",
    "#     img = cv2.imread(f'./test_data/{file}',cv2.IMREAD_GRAYSCALE)\n",
    "#     cv2.imwrite(f'./test_data/{file}',img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b6392bbf-7cda-4f11-8e31-2bed30a2cdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [googlenet,alexnet]\n",
    "for config in configs:\n",
    "    epochs = 125+31\n",
    "    model = TL_Model_Test(config[0]).to(device)\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adamax(model.parameters(),lr=0.0001)\n",
    "    wandb.init(project='Intred-or-Not-Product',name=f'{config[1]}-64')\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for i in range(0,len(X_train),64):\n",
    "            optimizer.zero_grad()\n",
    "            X_batch = X_train[i:i+BATCH_SIZE].view(-1,3,112,112).to(device)\n",
    "            y_batch = y_train[i:i+BATCH_SIZE].to(device)\n",
    "            model.to(device)\n",
    "            preds = model(X_batch)\n",
    "            preds.to(device)\n",
    "            loss = criterion(preds.view(-1,1).float(),y_batch.view(-1,1).float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            wandb.log({'loss_per_iter':loss.item()})\n",
    "        wandb.log({'loss_per_epoch':loss.item()})\n",
    "    torch.cuda.empty_cache()\n",
    "    preds_dict,idx_dict,preds_all,preds_final = best_pred(model=model,shape=True,transformed=True,mode='eval',directory='./1st-try/test_data_old/',preds=True,num_of_iters=100)\n",
    "    wandb.log({'test_accuracy_val_transformed':accuracy_preds_out(preds_final,truth)})\n",
    "    preds_dict,idx_dict,preds_all,preds_final = best_pred(model=model,shape=True,transformed=False,mode='eval',directory='./1st-try/test_data_old/',preds=True,num_of_iters=100)\n",
    "    wandb.log({'test_accuracy_val_not_transformed':accuracy_preds_out(preds_final,truth)})\n",
    "    torch.cuda.empty_cache()\n",
    "    wandb.log({'accuracy':accuracy(model,X_train[:1250],y_train[:1250])})\n",
    "    torch.cuda.empty_cache()\n",
    "    wandb.log({'val_accuracy':accuracy(model,X_test,y_test)})\n",
    "    wandb.finish()\n",
    "    best_pred(model=model,shape=True,transformed=True,mode='eval',directory='./1st-try/test_data_old/',preds=True)\n",
    "    print('-'*50)\n",
    "    best_pred(model=model,shape=True,transformed=False,mode='eval',directory='./1st-try/test_data_old/',preds=True) #88.4%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "73cc3799-deb5-4852-9b67-657416a5122c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_custom(configs={'model':[models.resnet18,models.resnet18],'batch_size':[128,64]}):\n",
    "    final = []\n",
    "    classes = list(configs.keys())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d6ebbb2d-d40a-4bcf-b59d-f39d00fa9736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model', 'batch_size']\n"
     ]
    }
   ],
   "source": [
    "grid_search_custom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "156715c8-96c7-4908-bb3d-d2d6182ade5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [{'model':googlenet,'batch_size':128,'criterion':nn.BCELoss(),'optimizer':torch.optim.Adam,'lr':0.0001},{'model':alexnet,'batch_size':128,'criterion':nn.BCELoss(),'optimizer':torch.optim.Adam,'lr':0.0001}]\n",
    "for config in configs:\n",
    "    epochs = 125+31\n",
    "    model = TL_Model_Test(config['model'][0]).to(device)\n",
    "    model.to(device)\n",
    "    criterion = config['criterion']\n",
    "    optimizer = config['optimizer'](model.parameters(),lr=config['lr'])\n",
    "    wandb.init(project='Intred-or-Not-Product',name=f'{config['model'][1]}-{config['batch_size']}-{config['criterion']}-{config['optimizer']}-{config['lr']}')\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for i in range(0,len(X_train),config['batch_size']):\n",
    "            optimizer.zero_grad()\n",
    "            X_batch = X_train[i:i+BATCH_SIZE].view(-1,3,112,112).to(device)\n",
    "            y_batch = y_train[i:i+BATCH_SIZE].to(device)\n",
    "            model.to(device)\n",
    "            preds = model(X_batch)\n",
    "            preds.to(device)\n",
    "            loss = criterion(preds.view(-1,1).float(),y_batch.view(-1,1).float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            wandb.log({'loss_per_iter':loss.item()})\n",
    "        wandb.log({'loss_per_epoch':loss.item()})\n",
    "    for _ in range(10):\n",
    "        torch.cuda.empty_cache()\n",
    "        preds_dict,idx_dict,preds_all,preds_final = best_pred(model=model,shape=True,transformed=True,mode='eval',directory='./1st-try/test_data_old/',preds=True,num_of_iters=125)\n",
    "        wandb.log({'test_accuracy_val_transformed':accuracy_preds_out(preds_final,truth)})\n",
    "        torch.cuda.empty_cache()\n",
    "        preds_dict,idx_dict,preds_all,preds_final = best_pred(model=model,shape=True,transformed=False,mode='eval',directory='./1st-try/test_data_old/',preds=True,num_of_iters=100)\n",
    "        wandb.log({'test_accuracy_val_not_transformed':accuracy_preds_out(preds_final,truth)})\n",
    "        torch.cuda.empty_cache()\n",
    "        test_acc_mean = np.mean([accuracy(model,get_image(model=model,shape=True,transformed=True,mode='eval'),truth,transformed=True),accuracy(model,get_image(model=model,shape=True,transformed=False,mode='eval'),truth,transformed=True)])\n",
    "        wamdb.log('test_accuracy':test_acc_mean)                        \n",
    "        torch.cuda.empty_cache()\n",
    "        wandb.log({'val_accuracy':accuracy(model,X_test,y_test)})\n",
    "    wandb.log({'accuracy':accuracy(model,X_train[:1250],y_train[:1250])})\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8411613c-3398-4c27-a8a3-8b6c542e4490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 125+31\n",
    "# model = TL_Model_Test(model=alexnet[0]).to(device)\n",
    "# model.to(device)\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adamax(model.parameters(),lr=0.0001)\n",
    "# wandb.init(project='Intred-or-Not-Product',name=f'test')\n",
    "# for epoch in tqdm(range(epochs)):\n",
    "#     for i in range(0,len(X_train),128):\n",
    "#         optimizer.zero_grad()\n",
    "#         X_batch = X_train[i:i+BATCH_SIZE].view(-1,3,112,112).to(device)\n",
    "#         y_batch = y_train[i:i+BATCH_SIZE].to(device)\n",
    "#         model.to(device)\n",
    "#         preds = model(X_batch)\n",
    "#         preds.to(device)\n",
    "#         loss = criterion(preds.view(-1,1).float(),y_batch.view(-1,1).float())\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         wandb.log({'loss_per_iter':loss.item()})\n",
    "#     wandb.log({'loss_per_epoch':loss.item()})\n",
    "# torch.cuda.empty_cache()\n",
    "# test_accuracy_val_transformed = []\n",
    "# for _ in range(10):\n",
    "#     preds_dict,idx_dict,preds_all,preds_final = best_pred(model=model,shape=True,transformed=True,mode='eval',directory='./1st-try/test_data_old/',preds=True,num_of_iters=100)\n",
    "#     test_accuracy_val_transformed.append(accuracy_preds_out(preds_final,truth))\n",
    "# wandb.log({'test_accuracy_val_transformed':np.mean(test_accuracy_val_transformed)})\n",
    "# preds_dict,idx_dict,preds_all,preds_final = best_pred(model=model,shape=True,transformed=False,mode='eval',directory='./1st-try/test_data_old/',preds=True,num_of_iters=100)\n",
    "# wandb.log({'test_accuracy_val_not_transformed':accuracy_preds_out(preds_final,truth)})\n",
    "# torch.cuda.empty_cache()\n",
    "# wandb.log({'accuracy':accuracy(model,X_train[:1250],y_train[:1250])})\n",
    "# torch.cuda.empty_cache()\n",
    "# wandb.log({'val_accuracy':accuracy(model,X_test,y_test)})\n",
    "# wandb.finish()\n",
    "# best_pred(model=model,shape=True,transformed=True,mode='eval',directory='./1st-try/test_data_old/',preds=True)\n",
    "# print('-'*50)\n",
    "# best_pred(model=model,shape=True,transformed=False,mode='eval',directory='./1st-try/test_data_old/',preds=True) #88.4%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8683ee30-0b25-419a-9d1d-76f1a5f5c328",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mranuga-d\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.32<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">156</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/ranuga-d/Intred-or-Not-Product\" target=\"_blank\">https://wandb.ai/ranuga-d/Intred-or-Not-Product</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/ranuga-d/Intred-or-Not-Product/runs/1drh7p1r\" target=\"_blank\">https://wandb.ai/ranuga-d/Intred-or-Not-Product/runs/1drh7p1r</a><br/>\n",
       "                Run data is saved locally in <code>/home/indika/Programming/Projects/Python/Artifical-Intelligence/PyTorch/Competions/Intrested-or-Not-Product/CNN/wandb/run-20210627_212556-1drh7p1r</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|         | 5/156 [01:19<40:01, 15.91s/it]"
     ]
    }
   ],
   "source": [
    "configs = [{'epochs':125+31},{'epochs':25},{'epochs':50},{'epochs':75},{'epochs':100},{'epochs':125},{'epochs':150},{'epochs':175},{'epochs':200},{'epochs':225},{'epochs':250},{'epochs':250+125},{'epochs':500}]\n",
    "for config in configs:\n",
    "    epochs = 125+31\n",
    "    model = TL_Model_Test(model=googlenet[0]).to(device)\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adamax(model.parameters(),lr=0.0001)\n",
    "    wandb.init(project='Intred-or-Not-Product',name=f'{config[\"epochs\"]}')\n",
    "    for epoch in tqdm(range(config['epochs'])):\n",
    "        for i in range(0,len(X_train),128):\n",
    "            optimizer.zero_grad()\n",
    "            X_batch = X_train[i:i+BATCH_SIZE].view(-1,3,112,112).to(device)\n",
    "            y_batch = y_train[i:i+BATCH_SIZE].to(device)\n",
    "            model.to(device)\n",
    "            preds = model(X_batch)\n",
    "            preds.to(device)\n",
    "            loss = criterion(preds.view(-1,1).float(),y_batch.view(-1,1).float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            wandb.log({'loss_per_iter':loss.item()})\n",
    "        wandb.log({'loss_per_epoch':loss.item()})\n",
    "    torch.cuda.empty_cache()\n",
    "    test_accuracy_val_transformed = []\n",
    "    for _ in range(10):\n",
    "        preds_dict,idx_dict,preds_all,preds_final = best_pred(model=model,shape=True,transformed=True,mode='eval',directory='./1st-try/test_data_old/',preds=True,num_of_iters=100)\n",
    "        test_accuracy_val_transformed.append(accuracy_preds_out(preds_final,truth))\n",
    "    wandb.log({'test_accuracy_val_transformed':np.mean(test_accuracy_val_transformed)})\n",
    "    preds_dict,idx_dict,preds_all,preds_final = best_pred(model=model,shape=True,transformed=False,mode='eval',directory='./1st-try/test_data_old/',preds=True,num_of_iters=100)\n",
    "    wandb.log({'test_accuracy_val_not_transformed':accuracy_preds_out(preds_final,truth)})\n",
    "    torch.cuda.empty_cache()\n",
    "    wandb.log({'accuracy':accuracy(model,X_train[:1250],y_train[:1250])})\n",
    "    torch.cuda.empty_cache()\n",
    "    wandb.log({'val_accuracy':accuracy(model,X_test,y_test)})\n",
    "    wandb.finish()\n",
    "    best_pred(model=model,shape=True,transformed=True,mode='eval',directory='./1st-try/test_data_old/',preds=True)\n",
    "    print('-'*50)\n",
    "    best_pred(model=model,shape=True,transformed=False,mode='eval',directory='./1st-try/test_data_old/',preds=True) #88.4%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd58773-9044-460a-897d-9488f43f2734",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f0474e-ff9c-41ce-8b71-5aa83e345e00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a6f9b2-73b4-4ea0-84bd-e5a261a9fd7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d48589-a127-4b4d-889c-4acff16b0c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python373jvsc74a57bd0210f9608a45c0278a93c9e0b10db32a427986ab48cfc0d20c139811eb78c4bbc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
